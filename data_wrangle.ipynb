{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import nltk\n",
    "import functools\n",
    "import re\n",
    "import json\n",
    "import statistics\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_COUNTS_NAME = 'word_counts'\n",
    "WORD_COUNTS_LIST_NAME = 'word_counts_list'\n",
    "TOTAL_NAME = 'total_words'\n",
    "COMMENTS_COLUMN = 'comments'\n",
    "FREQUENCY_NAME = 'frequency'\n",
    "MAGNIFY_LIST = ['too', 'very', 'that', 'so', 'as']\n",
    "NEGATIVE_LIST = ['not', 'aren\\'t', 'isn\\'t', 'wasn\\'t', 'didn\\'t']\n",
    "with open('constants/word_categories.json', 'r') as f:\n",
    "    WORD_CATEGORIES = json.load(f)\n",
    "IMPORTANT_WORDS = reduce(lambda x, y: x + y, WORD_CATEGORIES.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGH MEMORY: Don't load unless you have to.  The code to reproduces this is below.\n",
    "with open('overall_list.json', 'r') as f:\n",
    "    ALL_WORKLOAD_LIST = json.load(f)\n",
    "    \n",
    "# TODO: Finish this for other metrics\n",
    "# with open('overall_list.json', 'r') as f:\n",
    "#     ALL_RECOMMEND_LIST = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (54,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('raw_data/final.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get a list of words from a string sentence.'''\n",
    "def get_words(text):\n",
    "    return re.compile('\\w+').findall(text)\n",
    "\n",
    "'''See if a word was used in a negative context'''\n",
    "def probe_if_negative(word1, word2):\n",
    "    if word1 in NEGATIVE_LIST:\n",
    "        return True\n",
    "    elif (word2 in NEGATIVE_LIST) and (word1 in MAGNIFY_LIST):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_word_dict(comments, to_return):\n",
    "    d = {}\n",
    "    t = 0\n",
    "    for comment in comments:\n",
    "        last_word = ''\n",
    "        two_words_ago = ''\n",
    "        for word in get_words(comment):\n",
    "            word = word.lower() # Make lower case\n",
    "            t += 1\n",
    "            try:\n",
    "                if probe_if_negative(last_word, two_words_ago):\n",
    "                    d['not_' + word] += 1\n",
    "                else:\n",
    "                    d[word] += 1\n",
    "            except: \n",
    "                if probe_if_negative(last_word, two_words_ago):\n",
    "                    d['not_' + word] = 1\n",
    "                else:\n",
    "                    d[word] = 1\n",
    "            two_words_ago = last_word\n",
    "            last_word = word\n",
    "    if to_return == 'words':\n",
    "        return d\n",
    "    elif to_return == 'count':\n",
    "        return t\n",
    "\n",
    "def preprocessing(to_return, row):\n",
    "    raw_comments = row[COMMENTS_COLUMN]\n",
    "    return get_word_dict(ast.literal_eval(raw_comments), to_return)\n",
    "\n",
    "def count_words(in_df):\n",
    "    in_df = in_df.copy()\n",
    "    in_df[TOTAL_NAME] = in_df.apply(functools.partial(preprocessing, 'count'), axis=1)\n",
    "    in_df[WORD_COUNTS_NAME] = in_df.apply(functools.partial(preprocessing, 'words'), axis=1)\n",
    "    return in_df\n",
    "\n",
    "'''Calculates the frequency of a word in the comments of row'''\n",
    "def calculate_frequency(word, row):\n",
    "    try:\n",
    "        return float(row[WORD_COUNTS_NAME][word]) / float(row[TOTAL_NAME])\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "'''Weighted mean'''\n",
    "def weighted_mean(x, w):\n",
    "    return np.sum(x * w) / np.sum(w)\n",
    "\n",
    "'''Weighted covariance'''\n",
    "def weighted_cov(x, y, w):\n",
    "    return np.sum(w * (x - weighted_mean(x, w)) * (y - weighted_mean(y, w))) / np.sum(w)\n",
    "\n",
    "'''Weighted correlation'''\n",
    "def corr(x, y, w):\n",
    "    return weighted_cov(x, y, w) / np.sqrt(weighted_cov(x, x, w) * weighted_cov(y, y, w))\n",
    "\n",
    "''' Returns two column dataframe consisting of df[column] and the frequency of the word.\n",
    "    Weights according to the number of comments. '''\n",
    "def find_word_correlations(in_df, word, column):\n",
    "    freq = in_df.apply(functools.partial(calculate_frequency, word), axis=1)\n",
    "    possibly_nan = pd.concat([freq, in_df[column], in_df[COMMENTS_COLUMN]], \n",
    "                             axis=1, \n",
    "                             keys=[FREQUENCY_NAME, column, COMMENTS_COLUMN])\n",
    "    clean_df = possibly_nan.dropna(axis=0, how='any')\n",
    "    return corr(clean_df[FREQUENCY_NAME], \n",
    "                clean_df[column],\n",
    "                clean_df[COMMENTS_COLUMN].apply(lambda x: len(ast.literal_eval(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict_values(some_dict):\n",
    "    return sum(some_dict.values())\n",
    "\n",
    "'''Finds overall frequency of words in df by constructing total count and total dict'''\n",
    "def find_word_freqs_over_df(in_df, agg_method):\n",
    "    in_df = in_df.copy()\n",
    "    if agg_method == 'counter':\n",
    "        D = Counter({})\n",
    "        for index, row in in_df.iterrows():\n",
    "            D += Counter(row[WORD_COUNTS_NAME])\n",
    "        return dict(D)\n",
    "    elif agg_method == 'list':\n",
    "        D = {}\n",
    "        keys = set({})\n",
    "        for index, row in in_df.iterrows():\n",
    "            r = row[WORD_COUNTS_LIST_NAME]\n",
    "            \n",
    "            keys = keys.union(set(r.keys()))\n",
    "            for key in keys:\n",
    "                try:\n",
    "                    Dval = D[key]\n",
    "                except:\n",
    "                    Dval = []\n",
    "                try:\n",
    "                    rval = r[key]\n",
    "                except:\n",
    "                    rval = []\n",
    "                D[key] = Dval + rval\n",
    "        return D\n",
    "    \n",
    "'''Breaks df into subdfs, then finds frequencies for subdfs.'''\n",
    "def find_group_word_freqs(whole_df, gb):\n",
    "    whole_df = whole_df.copy()\n",
    "    gb = whole_df.groupby(gb)    \n",
    "    return [(x, find_word_freqs_over_df(gb.get_group(x))) for x in gb.groups]\n",
    "\n",
    "def create_group_name(tuple_or_string):\n",
    "    if type(tuple_or_string) == tuple:\n",
    "        return tuple_or_string[1] + '_' + str(tuple_or_string[0])\n",
    "    else:\n",
    "        return tuple_or_string\n",
    "\n",
    "'''Truncates to most frequent 50 words in each dict.  Puts dicts in '''\n",
    "def package_word_freqs(freqs_list_tuples):\n",
    "    output = {}\n",
    "    for category_name, freqs_list in freqs_list_tuples:\n",
    "        for grouping, freqs in freqs_list:\n",
    "            name = create_group_name(grouping)\n",
    "            freq_list = straighten_list(freqs, 50)\n",
    "            try:\n",
    "                output[category_name][name] = freq_list\n",
    "            except:\n",
    "                output[category_name] = {}\n",
    "                output[category_name][name] = freq_list\n",
    "    return output\n",
    "\n",
    "'''Turns dict k, v set into list and truncates to '''\n",
    "def straighten_list(in_dict, truncate_value):\n",
    "    return sorted(in_dict.items(), key=lambda word_count: -word_count[1])[:truncate_value]\n",
    "\n",
    "'''Turns a dict of occurence count form into a dict of target_value_list form'''\n",
    "def convert_dict(in_dict, value):\n",
    "    in_dict = dict(in_dict) # Make a copy so nothing bad happens\n",
    "    new_dict = {}\n",
    "    for k, v in in_dict.iteritems():\n",
    "        new_dict[k] = [value] * v\n",
    "    return new_dict\n",
    "\n",
    "'''Applies convert dict to each row of the in_df with values according to column'''\n",
    "def apply_list_conversion(in_df, column):\n",
    "    in_df = in_df.copy() # Copy so nothing bad happens\n",
    "    in_df[WORD_COUNTS_LIST_NAME] = (in_df[[WORD_COUNTS_NAME, column]]\n",
    "                                    .apply(lambda x: convert_dict(x[WORD_COUNTS_NAME], x[column]), axis=1))\n",
    "    return in_df\n",
    "\n",
    "def find_specific_word_values(word_list, metric):\n",
    "    D = {}\n",
    "    if metric == 'workload':\n",
    "        for word in word_list:\n",
    "            D[word] = [x for x in ALL_WORKLOAD_LIST[word] if not np.isnan(x)]\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = count_words(df)\n",
    "df3 = apply_list_conversion(df2, 'Course_Workload_Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_department = find_group_word_freqs(df2, 'department1')\n",
    "by_year = find_group_word_freqs(df2, ['year', 'term'])\n",
    "cdf_data = package_word_freqs([('department', by_department), ('year', by_year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_counter = find_word_freqs_over_df(df3, 'counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cdf_data.json', 'w') as outfile:\n",
    "        json.dump(cdf_data, outfile)\n",
    "\n",
    "with open('data/all_workload_list.json', 'w') as outfile:\n",
    "        json.dump(ALL_WORKLOAD_LIST, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/short_workload_list.json', 'w') as outfile:\n",
    "        json.dump(find_specific_word_values(IMPORTANT_WORDS, 'workload'), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Enrollment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL FUNCTIONS FOR THIS PART\n",
    "def filter_nans(val):\n",
    "    if val == [] or val == '[]':\n",
    "        return 0\n",
    "    else:\n",
    "        return float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "of_interest = df[['department1', 'year', 'term', \n",
    "                  'enrollment', 'name_key1', 'course_title']]\n",
    "of_interest.loc[:, 'enrollment'] = (of_interest['enrollment']\n",
    "                                     .apply(lambda x: filter_nans(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_depts = (of_interest.groupby('department1')['enrollment'].sum())\n",
    "top_depts_list = top_depts.astype('int').sort_values()[-30:].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_more_interest = (of_interest[of_interest['department1']\n",
    "                                .isin(top_depts_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_more_interest.to_csv('enrollment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_COLS = ['Course_Workload_Distribution', \n",
    "                 'Course_Overall_Distribution',\n",
    "                 'Course_Overall_Respondents',\n",
    "                 'Course_Workload_Rating',\n",
    "                 'Course_Overall_Rating',\n",
    "                 'Course_Workload_Respondents',\n",
    "                 'enrollment', \n",
    "                 'name_key1',\n",
    "                 'course_title', \n",
    "                 'year',\n",
    "                 'term',\n",
    "                 'department1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('constants/conventions.json', 'r') as f:\n",
    "    CONVENTIONS =  json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ith_list_val(i, lst):\n",
    "    if type(lst) == type(np.nan):\n",
    "        return 0\n",
    "    else:\n",
    "        lst = ast.literal_eval(lst)\n",
    "        if len(lst) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return lst[i]\n",
    "        \n",
    "def get_list_stat_nice(lst, func, std_multiplier):\n",
    "    big_list = []\n",
    "    for i in range(len(lst)):\n",
    "        big_list += ([i] * lst[i])\n",
    "    if len(big_list) == 1 and func == statistics.stdev: # Need 2+ data pts for variance\n",
    "        return -1\n",
    "    if func == statistics.stdev:\n",
    "        return func(big_list) * std_multiplier\n",
    "    else:\n",
    "        return func(big_list)\n",
    "\n",
    "def get_list_stat(lst, func, std_multiplier):\n",
    "    if type(lst) == type(np.nan):\n",
    "        return -1\n",
    "    else:\n",
    "        lst = ast.literal_eval(lst)\n",
    "        if len(lst) == 0 or reduce(lambda x, y: x+y, lst) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return get_list_stat_nice(lst, func, std_multiplier)\n",
    "\n",
    "def get_stat_score(statistic, desired, lst):\n",
    "    if desired == 'median':\n",
    "        intervals = CONVENTIONS[statistic]['intervals']\n",
    "        m = get_list_stat(lst, statistics.median, 1)\n",
    "        if m == -1:\n",
    "            return np.nan\n",
    "        if float(m).is_integer():\n",
    "            i = int(m)\n",
    "            return 0.5 * (intervals[i]['max'] + intervals[i]['min'])\n",
    "        else:\n",
    "            m = int(m-0.5)\n",
    "            return 0.5 * (intervals[m]['max'] + intervals[m + 1]['min'])\n",
    "    elif desired == 'stdev':\n",
    "        std_multiplier = CONVENTIONS[statistic]['std_multiplier']\n",
    "        return get_list_stat(lst, statistics.stdev, std_multiplier)\n",
    "\n",
    "def add_distribution_data(in_df, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    for statistic in statistics:\n",
    "        column = CONVENTIONS[statistic]['distribution']\n",
    "        for i in range(5):\n",
    "            in_df[statistic + '_s' + str(i+1)] = (in_df[column]\n",
    "                .apply(functools.partial(get_ith_list_val, i)))\n",
    "    return in_df\n",
    "\n",
    "def add_mean_data(in_df, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    for statistic in statistics:\n",
    "        in_df[statistic + '_mean'] = in_df[CONVENTIONS[statistic]['mean']]\n",
    "    return in_df\n",
    "\n",
    "def add_respondents_data(in_df, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    for statistic in statistics:\n",
    "        in_df[statistic + '_respondents'] = in_df[CONVENTIONS[statistic]['respondents']]\n",
    "    return in_df\n",
    "\n",
    "def add_instdev_data(in_df, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    for statistic in statistics:\n",
    "        column = CONVENTIONS[statistic]['distribution']\n",
    "        in_df[statistic + '_instdev'] = (in_df[column]\n",
    "                                        .apply(functools.partial(get_stat_score, statistic, 'stdev')))\n",
    "    return in_df\n",
    "\n",
    "\n",
    "def add_additional_data(in_df, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    in_df = add_respondents_data(in_df, statistics)\n",
    "    in_df = add_distribution_data(in_df, statistics)\n",
    "    in_df = add_mean_data(in_df, statistics)\n",
    "    in_df = add_instdev_data(in_df, statistics)\n",
    "    return in_df\n",
    "\n",
    "'''Computes weighted mean'''\n",
    "def compute_grouped_mean(in_df, val_col, weight_col):\n",
    "    in_df = in_df.copy()\n",
    "    return weighted_mean(x=in_df[val_col], w=in_df[weight_col])\n",
    "\n",
    "'''Computes the inter-group std (std of means weighted according to num respondants)'''\n",
    "def compute_grouped_outstd(in_df, mean_col, weight_col):\n",
    "    in_df = in_df.copy()\n",
    "    return np.sqrt(weighted_cov(w=in_df[weight_col], x=in_df[mean_col], y=in_df[mean_col]))\n",
    "\n",
    "def add_grouped_data(in_df, gb, statistics):\n",
    "    in_df = in_df.copy()\n",
    "    distribution_columns = []\n",
    "    for statistic in statistics:\n",
    "        for num in [1, 2, 3, 4, 5]:\n",
    "            distribution_columns += [statistic + '_s' + str(num)]\n",
    "    gp_df = in_df.groupby(gb)[distribution_columns].sum()\n",
    "    for statistic in statistics:\n",
    "        gp_df = (gp_df.join(in_df.groupby(gb)[[statistic + '_mean', statistic + '_respondents']]\n",
    "                           .apply(compute_grouped_mean, statistic + '_mean', statistic + '_respondents')\n",
    "                           .to_frame(statistic + '_mean'))\n",
    "                 .join(in_df.groupby(gb)[[statistic + '_instdev', statistic + '_respondents']]\n",
    "                       .apply(compute_grouped_mean, statistic + '_instdev', statistic + '_respondents')\n",
    "                       .to_frame(statistic + '_instdev'))\n",
    "                 .join(in_df.groupby(gb)[[statistic + '_mean', statistic + '_respondents']]\n",
    "                       .apply(compute_grouped_outstd, statistic + '_mean', statistic + '_respondents')\n",
    "                       .to_frame(statistic + '_outstdev')))\n",
    "    return gp_df\n",
    "\n",
    "def make_all_stats_for_year(in_df, year):\n",
    "    in_df = in_df.copy()\n",
    "    relevant_df = in_df[in_df.year == year][RELEVANT_COLS]\n",
    "    basic_stats_df = add_additional_data(relevant_df, ['workload', 'overall'])\n",
    "    grouped_stats_df = add_grouped_data(basic_stats_df, 'department1', ['workload', 'overall'])\n",
    "    grouped_stats_df = grouped_stats_df[grouped_stats_df.index.isin(top_depts_list)]\n",
    "    grouped_stats_df['year'] = year\n",
    "    return grouped_stats_df.reset_index().set_index(['year', 'department1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = pd.concat(make_all_stats_for_year(df, year) for year in df.year.unique().tolist())\n",
    "all_stats.to_csv('data/statistics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
